{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epub_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_epub_working/\"\n",
    "txt_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_txt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to extract text from epub\n",
    "\n",
    "def convert_epub_to_text(epub_path, epub_file, txt_path):\n",
    "\n",
    "    clean_text = ''\n",
    "    text_name = epub_file.replace(' ','_')[:-4]+'txt'\n",
    "    \n",
    "    # extract text from epub\n",
    "    text = textract.process(epub_path+epub_file,encoding='utf_8')\n",
    "    \n",
    "    # trip out the unicode and return characters\n",
    "    # still working on the \\ characters\n",
    "    #for i in text.split(' '):\n",
    "    clean_text = text.decode('ascii', 'ignore').replace('\\n',' ')\n",
    "    \n",
    "    # save as text file\n",
    "    text_file = open(txt_path+text_name, 'w')\n",
    "    text_file.write(clean_text)\n",
    "    text_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacred Games - Vikram Chandra.epub  failed\n"
     ]
    }
   ],
   "source": [
    "# loop through files in directory, convert file, save file in new folder\n",
    "\n",
    "for epub in os.listdir(epub_path):\n",
    "    try:\n",
    "        convert_epub_to_text(epub_path, epub, txt_path)\n",
    "    except:\n",
    "        print epub, \" failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load txt files into dataframe, \n",
    "# give each entry a best_selling 1/0 entry and a sci_fi 1/0 (0=romance) indicator\n",
    "\n",
    "path = '/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/'\n",
    "folders = ['sci-fi_top','sci-fi_flop','romance_top','romance_flop']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    if folder[-3:]=='top':\n",
    "        bs = 1\n",
    "    else:\n",
    "        bs = 0\n",
    "    if folder[:3]=='sci':\n",
    "        sf = 1\n",
    "    else:\n",
    "        sf = 0\n",
    "        \n",
    "    for text_file in os.listdir(path+folder+'/'):\n",
    "        full_path = path + folder + '/' + text_file\n",
    "        if text_file.endswith((\".txt\")):\n",
    "            text  = open(full_path, 'r').read()\n",
    "            temp = pd.DataFrame({\n",
    "                    'best_seller': bs,\n",
    "                    'sci_fi': sf,\n",
    "                    'title': text_file[:-4].replace('_',' ').replace('-',' - '),\n",
    "                    'body': text.decode('ascii', 'ignore').replace('\\n',' ').replace('\\r','')}, \n",
    "                                index=[0])\n",
    "            df = pd.concat([df, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_seller</th>\n",
       "      <th>body</th>\n",
       "      <th>sci_fi</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How to explain? How to describe? Even the omni...</td>\n",
       "      <td>1</td>\n",
       "      <td>a fire upon the deep - vernor vinge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The body lay naked and facedown, a deathly gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>ancillary justice - ann leckie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_seller                                               body  sci_fi  \\\n",
       "0            1  How to explain? How to describe? Even the omni...       1   \n",
       "1            1   The body lay naked and facedown, a deathly gr...       1   \n",
       "\n",
       "                                 title  \n",
       "0  a fire upon the deep - vernor vinge  \n",
       "1       ancillary justice - ann leckie  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fire upon the deep - vernor vinge  :  appendix\n",
      "dune - frank herbert  :  appendix\n",
      "rainbows end - vernor vinge  :  table of contents\n",
      "rainbows end - vernor vinge  :  appendix\n",
      "rainbows end - vernor vinge  :  copyright\n",
      "the algebraist - iain m banks  :  acknowledgements\n",
      "the algebraist - iain m banks  :  appendix\n",
      "atlanta nights - travis tea  :  appendix\n",
      "fifty shades of grey - e l james  :  appendix\n",
      "grey - e l james  :  appendix\n",
      "outlander voyager - diana galbaldon  :  appendix\n"
     ]
    }
   ],
   "source": [
    "# check for front and back matter in body\n",
    "# i have cleaned up the files by using this alert\n",
    "# remaining issues are intentional usage of terms in the text\n",
    "\n",
    "check_words = ['acknowledgements','table of contents','about the author', 'appendix', \n",
    "               'copyright','isbn','by this author']\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    for word in check_words:\n",
    "        if word in df.iloc[i,1].lower():\n",
    "            print df.ix[i,3], ' : ', word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_sentence_len(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_counts = []\n",
    "    sentences = re.split(r'[.|!|?]', text)\n",
    "    for sentence in sentences[:-1]:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        word_counts.append(len(words))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)  \n",
    "    return avg_word_count\n",
    "\n",
    "def word_count(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def avg_word_len(text):\n",
    "    letter_counts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    for word in words:\n",
    "        letter_counts.append(len(word))\n",
    "    avg_word_len = sum(letter_counts)/len(letter_counts)\n",
    "    return avg_word_len\n",
    "\n",
    "def lex_div(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    lexical_diversity = 1.0 * len(set(words)) / len(words)\n",
    "    return lexical_diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['avg_sent_len'] = df['body'].apply(avg_sentence_len)\n",
    "df['word_count'] = df['body'].apply(word_count)\n",
    "df['avg_word_len'] = df['body'].apply(avg_word_len)\n",
    "df['lex_diversity'] = df['body'].apply(lex_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_seller</th>\n",
       "      <th>body</th>\n",
       "      <th>sci_fi</th>\n",
       "      <th>title</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lex_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How to explain? How to describe? Even the omni...</td>\n",
       "      <td>1</td>\n",
       "      <td>a fire upon the deep - vernor vinge</td>\n",
       "      <td>11</td>\n",
       "      <td>207548</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The body lay naked and facedown, a deathly gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>ancillary justice - ann leckie</td>\n",
       "      <td>11</td>\n",
       "      <td>108263</td>\n",
       "      <td>4</td>\n",
       "      <td>0.079815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Children of Dune     Frank Herbert   Muad'D...</td>\n",
       "      <td>1</td>\n",
       "      <td>children of dune - frank herbert</td>\n",
       "      <td>10</td>\n",
       "      <td>151352</td>\n",
       "      <td>4</td>\n",
       "      <td>0.076094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Consider Phlebas  Iain Banks  Idolatry is wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>consider phlebas - iain m banks</td>\n",
       "      <td>14</td>\n",
       "      <td>176337</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>By nature, men are nearly alike; by practic...</td>\n",
       "      <td>1</td>\n",
       "      <td>diamond age - neal stephenson</td>\n",
       "      <td>17</td>\n",
       "      <td>190498</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_seller                                               body  sci_fi  \\\n",
       "0            1  How to explain? How to describe? Even the omni...       1   \n",
       "1            1   The body lay naked and facedown, a deathly gr...       1   \n",
       "2            1     Children of Dune     Frank Herbert   Muad'D...       1   \n",
       "3            1   Consider Phlebas  Iain Banks  Idolatry is wor...       1   \n",
       "4            1     By nature, men are nearly alike; by practic...       1   \n",
       "\n",
       "                                 title  avg_sent_len  word_count  \\\n",
       "0  a fire upon the deep - vernor vinge            11      207548   \n",
       "1       ancillary justice - ann leckie            11      108263   \n",
       "2     children of dune - frank herbert            10      151352   \n",
       "3      consider phlebas - iain m banks            14      176337   \n",
       "4        diamond age - neal stephenson            17      190498   \n",
       "\n",
       "   avg_word_len  lex_diversity  \n",
       "0             4       0.064477  \n",
       "1             4       0.079815  \n",
       "2             4       0.076094  \n",
       "3             4       0.064042  \n",
       "4             4       0.085061  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best_seller</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci_fi</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.502096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sent_len</th>\n",
       "      <td>35.0</td>\n",
       "      <td>11.514286</td>\n",
       "      <td>2.800960</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>35.0</td>\n",
       "      <td>136649.114286</td>\n",
       "      <td>73070.362563</td>\n",
       "      <td>25869.000000</td>\n",
       "      <td>91640.000000</td>\n",
       "      <td>120326.000000</td>\n",
       "      <td>174122.000000</td>\n",
       "      <td>394034.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_len</th>\n",
       "      <td>35.0</td>\n",
       "      <td>3.971429</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lex_diversity</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.086743</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.056274</td>\n",
       "      <td>0.071746</td>\n",
       "      <td>0.081857</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>0.16398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count           mean           std           min           25%  \\\n",
       "best_seller     35.0       0.971429      0.169031      0.000000      1.000000   \n",
       "sci_fi          35.0       0.571429      0.502096      0.000000      0.000000   \n",
       "avg_sent_len    35.0      11.514286      2.800960      7.000000     10.000000   \n",
       "word_count      35.0  136649.114286  73070.362563  25869.000000  91640.000000   \n",
       "avg_word_len    35.0       3.971429      0.169031      3.000000      4.000000   \n",
       "lex_diversity   35.0       0.086743      0.023328      0.056274      0.071746   \n",
       "\n",
       "                         50%            75%           max  \n",
       "best_seller         1.000000       1.000000       1.00000  \n",
       "sci_fi              1.000000       1.000000       1.00000  \n",
       "avg_sent_len       11.000000      13.000000      20.00000  \n",
       "word_count     120326.000000  174122.000000  394034.00000  \n",
       "avg_word_len        4.000000       4.000000       4.00000  \n",
       "lex_diversity       0.081857       0.098377       0.16398  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfbs = df[((df['best_seller']==1) & (df['sci_fi']==1))]\n",
    "rmbs = df[((df['best_seller']==1) & (df['sci_fi']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best_seller</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci_fi</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sent_len</th>\n",
       "      <td>19.0</td>\n",
       "      <td>11.421053</td>\n",
       "      <td>2.316985</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>19.0</td>\n",
       "      <td>141674.000000</td>\n",
       "      <td>47392.036349</td>\n",
       "      <td>61927.000000</td>\n",
       "      <td>105744.00000</td>\n",
       "      <td>147721.000000</td>\n",
       "      <td>182357.500000</td>\n",
       "      <td>211455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_len</th>\n",
       "      <td>19.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lex_diversity</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.086911</td>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.064042</td>\n",
       "      <td>0.07218</td>\n",
       "      <td>0.081857</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>0.121175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count           mean           std           min           25%  \\\n",
       "best_seller     19.0       1.000000      0.000000      1.000000       1.00000   \n",
       "sci_fi          19.0       1.000000      0.000000      1.000000       1.00000   \n",
       "avg_sent_len    19.0      11.421053      2.316985      9.000000      10.00000   \n",
       "word_count      19.0  141674.000000  47392.036349  61927.000000  105744.00000   \n",
       "avg_word_len    19.0       4.000000      0.000000      4.000000       4.00000   \n",
       "lex_diversity   19.0       0.086911      0.018780      0.064042       0.07218   \n",
       "\n",
       "                         50%            75%            max  \n",
       "best_seller         1.000000       1.000000       1.000000  \n",
       "sci_fi              1.000000       1.000000       1.000000  \n",
       "avg_sent_len       10.000000      13.000000      17.000000  \n",
       "word_count     147721.000000  182357.500000  211455.000000  \n",
       "avg_word_len        4.000000       4.000000       4.000000  \n",
       "lex_diversity       0.081857       0.098377       0.121175  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best_seller</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci_fi</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sent_len</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>3.480558</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>15.0</td>\n",
       "      <td>134483.200000</td>\n",
       "      <td>98774.725530</td>\n",
       "      <td>25869.000000</td>\n",
       "      <td>91640.000000</td>\n",
       "      <td>100512.000000</td>\n",
       "      <td>152186.000000</td>\n",
       "      <td>394034.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_len</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lex_diversity</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.083731</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.056274</td>\n",
       "      <td>0.068227</td>\n",
       "      <td>0.074736</td>\n",
       "      <td>0.088196</td>\n",
       "      <td>0.16398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count           mean           std           min           25%  \\\n",
       "best_seller     15.0       1.000000      0.000000      1.000000      1.000000   \n",
       "sci_fi          15.0       0.000000      0.000000      0.000000      0.000000   \n",
       "avg_sent_len    15.0      11.600000      3.480558      7.000000      9.000000   \n",
       "word_count      15.0  134483.200000  98774.725530  25869.000000  91640.000000   \n",
       "avg_word_len    15.0       3.933333      0.258199      3.000000      4.000000   \n",
       "lex_diversity   15.0       0.083731      0.027064      0.056274      0.068227   \n",
       "\n",
       "                         50%            75%           max  \n",
       "best_seller         1.000000       1.000000       1.00000  \n",
       "sci_fi              0.000000       0.000000       0.00000  \n",
       "avg_sent_len       11.000000      13.000000      20.00000  \n",
       "word_count     100512.000000  152186.000000  394034.00000  \n",
       "avg_word_len        4.000000       4.000000       4.00000  \n",
       "lex_diversity       0.074736       0.088196       0.16398  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial feature review to see anything interesting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['said','didnt','know','don','dont','didn','did','ve','like',\n",
    "                                           'enterprises','grey','ceo','232','le','christian'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,3), stop_words=stop_words)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hock seng', 579),\n",
       " (u'shook head', 470),\n",
       " (u'lieutenant awn', 397),\n",
       " (u'old man', 302),\n",
       " (u'long time', 266),\n",
       " (u'bene gesserit', 265),\n",
       " (u'princess nell', 242),\n",
       " (u'white shirts', 241),\n",
       " (u'years ago', 232),\n",
       " (u'judge fang', 210),\n",
       " (u'anaander mianaai', 201),\n",
       " (u'hiro says', 187),\n",
       " (u'muad dib', 181),\n",
       " (u'feyd rautha', 179),\n",
       " (u'thousand years', 171),\n",
       " (u'far away', 160),\n",
       " (u'reverend mother', 147),\n",
       " (u'pham nuwen', 147),\n",
       " (u'hong kong', 140),\n",
       " (u'uncle enzo', 132)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sci-fi books over-use character names\n",
    "# also a long time ago, far far away\n",
    "Counter(ngrams_summaries_sfbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 543),\n",
       " (u'miss steele', 468),\n",
       " (u'im going', 367),\n",
       " (u'anastasia steele', 323),\n",
       " (u'mr rochester', 286),\n",
       " (u'deep breath', 255),\n",
       " (u'im sorry', 251),\n",
       " (u'mrs jennings', 230),\n",
       " (u'young ian', 220),\n",
       " (u'im sure', 213),\n",
       " (u'tara jean', 206),\n",
       " (u'mr willoughby', 200),\n",
       " (u'long time', 198),\n",
       " (u'closed eyes', 197),\n",
       " (u'blue eyes', 173),\n",
       " (u'youre going', 164),\n",
       " (u'took deep', 158),\n",
       " (u'took deep breath', 140),\n",
       " (u'shaking head', 133),\n",
       " (u'yes sir', 132),\n",
       " (u'open door', 132),\n",
       " (u'colonel brandon', 130),\n",
       " (u'great deal', 130),\n",
       " (u'living room', 129),\n",
       " (u'steele subject', 127),\n",
       " (u'st john', 125),\n",
       " (u'young man', 123),\n",
       " (u'mrs dashwood', 121),\n",
       " (u'anastasia steele subject', 121),\n",
       " (u'turned away', 113)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# romance titles have some classics --> deep breath, closed eyes, emotional gestures\n",
    "Counter(ngrams_summaries_rmbs).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "char_name_stop = stop_words.union(['awn','muad','dib','hock','seng','nell','pham','nuwen', 'enzo','anaander','mianaai',\n",
    "                                   'rautha','feyd','willoughby','brandon','john','dashwood','jessica','fang','hiro',\n",
    "                                  'steele','anastasia','rochester','jennings','ian','middleton','tara','jean','fairfax',\n",
    "                                  'mcgraw','finkle'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,3), stop_words=char_name_stop)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 470),\n",
       " (u'old man', 302),\n",
       " (u'long time', 266),\n",
       " (u'bene gesserit', 265),\n",
       " (u'white shirts', 241),\n",
       " (u'years ago', 232),\n",
       " (u'thousand years', 171),\n",
       " (u'far away', 160),\n",
       " (u'reverend mother', 147),\n",
       " (u'hong kong', 140),\n",
       " (u'shakes head', 124),\n",
       " (u'make sure', 124),\n",
       " (u'sjandra kei', 121),\n",
       " (u'lord radch', 120),\n",
       " (u'closed eyes', 119),\n",
       " (u'long ago', 119),\n",
       " (u'right hand', 116),\n",
       " (u'half hour', 115),\n",
       " (u'years old', 114),\n",
       " (u'im sure', 106),\n",
       " (u'unaha closp', 105),\n",
       " (u'time time', 103),\n",
       " (u'deep breath', 102),\n",
       " (u'light years', 100),\n",
       " (u'young man', 99),\n",
       " (u'half dozen', 99),\n",
       " (u'turned away', 98),\n",
       " (u'fwi song', 97),\n",
       " (u'quercer janath', 96),\n",
       " (u'inspector supervisor', 96)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sfbs).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 543),\n",
       " (u'im going', 367),\n",
       " (u'deep breath', 255),\n",
       " (u'im sorry', 251),\n",
       " (u'im sure', 213),\n",
       " (u'long time', 198),\n",
       " (u'closed eyes', 197),\n",
       " (u'blue eyes', 173),\n",
       " (u'youre going', 164),\n",
       " (u'took deep', 158),\n",
       " (u'took deep breath', 140),\n",
       " (u'shaking head', 133),\n",
       " (u'yes sir', 132),\n",
       " (u'open door', 132),\n",
       " (u'great deal', 130),\n",
       " (u'living room', 129),\n",
       " (u'young man', 123),\n",
       " (u'turned away', 113),\n",
       " (u'oh aye', 112),\n",
       " (u'oh yes', 108),\n",
       " (u'eyes closed', 105),\n",
       " (u'eyes fixed', 101),\n",
       " (u'ive seen', 100),\n",
       " (u'im glad', 96),\n",
       " (u'ye ken', 94),\n",
       " (u'new york', 92),\n",
       " (u'little bit', 92),\n",
       " (u'years ago', 88),\n",
       " (u'gray eyes', 88),\n",
       " (u'cleared throat', 87)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is funnier\n",
    "Counter(ngrams_summaries_rmbs).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
