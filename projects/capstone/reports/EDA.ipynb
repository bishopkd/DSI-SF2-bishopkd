{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import textract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epub_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_epub_working/\"\n",
    "txt_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_txt/\"\n",
    "path = '/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/'\n",
    "folders = ['sci-fi_top','sci-fi_flop','romance_top','romance_flop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to extract text from epub\n",
    "def convert_epub_to_text(epub_path, epub_file, txt_path):\n",
    "    clean_text = ''\n",
    "    text_name = epub_file.replace(' ','_')[:-4]+'txt' #clean up filename and change file extention\n",
    "    \n",
    "    text = textract.process(epub_path+epub_file,encoding='utf_8') #extract text from epub\n",
    "    clean_text = text.decode('ascii', 'ignore').replace('\\n',' ') #trip out the unicode and return characters\n",
    "\n",
    "    text_file = open(txt_path+text_name, 'w') #save as text file\n",
    "    text_file.write(clean_text)\n",
    "    text_file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through files in directory, convert file, save file in new folder\n",
    "for epub in os.listdir(epub_path):\n",
    "    try:\n",
    "        convert_epub_to_text(epub_path, epub, txt_path)\n",
    "    except:\n",
    "        print epub, \"failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load txt files into dataframe, \n",
    "# give each entry a best_selling 1/0 entry and a sci_fi 1/0 (0=romance) indicator\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    if folder[-3:]=='top':\n",
    "        bs = 1\n",
    "    else:\n",
    "        bs = 0\n",
    "    if folder[:3]=='sci':\n",
    "        sf = 1\n",
    "    else:\n",
    "        sf = 0\n",
    "        \n",
    "    for text_file in os.listdir(path+folder+'/'):\n",
    "        full_path = path + folder + '/' + text_file\n",
    "        if text_file.endswith((\".txt\")):\n",
    "            text  = open(full_path, 'r').read()\n",
    "            temp = pd.DataFrame({\n",
    "                    'best_seller': bs,\n",
    "                    'sci_fi': sf,\n",
    "                    'title': text_file[:-4].replace('_',' ').replace('-',' - '),\n",
    "                    'body': text.decode('ascii', 'ignore').replace('\\n',' ').replace('\\r','')}, \n",
    "                                index=[0])\n",
    "            df = pd.concat([df, temp])\n",
    "            \n",
    "df = df.reset_index() # because index=[0]\n",
    "del df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fire upon the deep - vernor vinge  :  appendix\n",
      "dune - frank herbert  :  appendix\n",
      "rainbows end - vernor vinge  :  table of contents\n",
      "rainbows end - vernor vinge  :  appendix\n",
      "rainbows end - vernor vinge  :  copyright\n",
      "the algebraist - iain m banks  :  acknowledgements\n",
      "the algebraist - iain m banks  :  appendix\n",
      "atlanta nights - travis tea  :  appendix\n",
      "hell ship - philip palmer  :  about the author\n",
      "fifty shades of grey - e l james  :  appendix\n",
      "grey - e l james  :  appendix\n",
      "outlander voyager - diana galbaldon  :  appendix\n"
     ]
    }
   ],
   "source": [
    "# check for front and back matter in body\n",
    "# i have cleaned up the files by using this alert\n",
    "# remaining issues are intentional usage in the body\n",
    "\n",
    "check_words = ['acknowledgements','table of contents','about the author', 'appendix', \n",
    "               'copyright','isbn','by this author', 'chapter']\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    for word in check_words:\n",
    "        if word in df.iloc[i,1].lower():\n",
    "            print df.ix[i,3], ' : ', word        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load profanity file\n",
    "curses = pd.read_csv(path + 'other/profanity.csv')\n",
    "curses.drop('Unnamed: 1', inplace=True, axis=1)\n",
    "bad_words = curses.word.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function bank for creating metrics\n",
    "\n",
    "def avg_sentence_len(text):\n",
    "    word_counts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sent_detect = PunktSentenceTokenizer()\n",
    "    sentences = sent_detect.sentences_from_text(text)\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        word_counts.append(len(words))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)  \n",
    "    return avg_word_count\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "def get_token_words(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    return words\n",
    "\n",
    "def word_count(text):\n",
    "    words = get_token_words(text)\n",
    "    return len(words)\n",
    "\n",
    "def avg_word_len(text):\n",
    "    letter_counts = []\n",
    "    words = get_token_words(text)\n",
    "    for word in words:\n",
    "        letter_counts.append(len(word))\n",
    "    avg_word_len = sum(letter_counts)/len(letter_counts)\n",
    "    return avg_word_len\n",
    "\n",
    "def profanity_counter(text):\n",
    "    i=0\n",
    "    words = get_token_words(text)\n",
    "    for word in words:\n",
    "        if word in bad_words:\n",
    "            i+=1       \n",
    "    return i\n",
    "\n",
    "def lex_div(text):\n",
    "    words = get_token_words(text)\n",
    "    lexical_diversity = 1.0 * len(set(words)) / len(words)\n",
    "    return lexical_diversity\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "def to_blob(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob\n",
    "\n",
    "def assign_polarity(text):\n",
    "    blob = to_blob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def assign_subjectivity(text):\n",
    "    blob = to_blob(text)\n",
    "    return blob.sentiment.subjectivity\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "def parse_pos(df,field):\n",
    "    for i in range(0,len(df)):\n",
    "        blob = TextBlob(df.ix[i,field])\n",
    "        tags = blob.tags\n",
    "        df_tags = pd.DataFrame(tags)\n",
    "        df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "        for x in range(0,len(df_tags)):\n",
    "                df.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "        df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new columns of metrics\n",
    "# note: this is take a very long time to run\n",
    "\n",
    "df['avg_sent_len'] = df['body'].map(avg_sentence_len)\n",
    "df['word_count'] = df['body'].map(word_count)\n",
    "df['avg_word_len'] = df['body'].map(avg_word_len)\n",
    "df['lex_diversity'] = df['body'].map(lex_div)\n",
    "df['polarity'] = df['body'].map(assign_polarity)\n",
    "df['subjectivity'] = df['body'].map(assign_subjectivity)\n",
    "df['profanity'] = df['body'].map(profanity_counter)\n",
    "df['profane'] = 1. * df['profanity']/df['word_count']\n",
    "parse_pos(df,'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'CC':'conj_coord', 'CD':'number', 'DT':'determiner', 'EX':'exist_there',\n",
    "                  'FW':'foreign_word','IN':'conj_sub_prep','JJ':'adj','JJR':'adj_compare',\n",
    "                 'JJS':'adj_sup','MD':'verb_aux',  'NN':'noun','NNP':'noun_prop',\n",
    "                'NNPS':'noun_prop_pural',  'NNS':'noun_plural', 'PDT':'predeterm','PRP':'pronoun_pers',\n",
    "                'PRP$':'pronoun_poss',  'RB':'adv','RBR':'adv_compare','RBS':'adv_sup',\n",
    "                  'RP':'adv_part', 'TO':'inf_to',  'UH':'interject','VB':'verb_base',\n",
    "                 'VBD':'verb_past','VBG':'verb_ger','VBN':'verb_pp','VBP':'verb_sing_pres',\n",
    "                 'VBZ':'verb_3rd_sing_pres','WDT':'wh_determ','WP':'wh_pronoun','WP$':'wh_poss',\n",
    "                 'WRB':'wh_adv','POS':'poss_ending','SYM':'symbol','LS':'list_marker'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_seller</th>\n",
       "      <th>body</th>\n",
       "      <th>sci_fi</th>\n",
       "      <th>title</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lex_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How to explain? How to describe? Even the omni...</td>\n",
       "      <td>1</td>\n",
       "      <td>a fire upon the deep - vernor vinge</td>\n",
       "      <td>11</td>\n",
       "      <td>207548</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The body lay naked and facedown, a deathly gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>ancillary justice - ann leckie</td>\n",
       "      <td>11</td>\n",
       "      <td>108263</td>\n",
       "      <td>4</td>\n",
       "      <td>0.079815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_seller                                               body  sci_fi  \\\n",
       "0            1  How to explain? How to describe? Even the omni...       1   \n",
       "1            1   The body lay naked and facedown, a deathly gr...       1   \n",
       "\n",
       "                                 title  avg_sent_len  word_count  \\\n",
       "0  a fire upon the deep - vernor vinge            11      207548   \n",
       "1       ancillary justice - ann leckie            11      108263   \n",
       "\n",
       "   avg_word_len  lex_diversity  \n",
       "0             4       0.064477  \n",
       "1             4       0.079815  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path + 'df.csv') # save to file so we don't have to do that again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = df[(df['best_seller']==1)]\n",
    "f = df[(df['best_seller']==0)]\n",
    "sfbs = df[((df['best_seller']==1) & (df['sci_fi']==1))]\n",
    "rmbs = df[((df['best_seller']==1) & (df['sci_fi']==0))]\n",
    "sff = df[((df['best_seller']==0) & (df['sci_fi']==1))]\n",
    "rmf = df[((df['best_seller']==0) & (df['sci_fi']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sff.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmf.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union(['232'])\n",
    "char_name_stop = stop_words.union(['awn','muad','dib','hock','seng','nell','pham','nuwen', 'enzo','anaander','mianaai',\n",
    "                                   'rautha','feyd','willoughby','brandon','john','dashwood','jessica','fang','hiro',\n",
    "                                  'steele','anastasia','rochester','jennings','ian','middleton','tara','jean','fairfax',\n",
    "                                  'mcgraw','finkle', 'dearborne','dearbornes','peterby','anne','alice','henry','simon',\n",
    "                                  'gavin','marco','bruce','catherine','nicky','brent','reverend','bene','gesserit','clef',\n",
    "                                  'radch','kei',\"sjandra\",'goodbody','paul','robert','horza','ender','ruby','travis','dar',\n",
    "                                  'miller','holden','travis','leto','mike','anna','justus','swan','jeff','onor','stilgar',\n",
    "                                  'sam','jamie','sasha','riley','nerezza','christian','grey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial feature review to see anything interesting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), stop_words=char_name_stop)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "summaries_sff = \"\".join(sff['body'])\n",
    "summaries_rmf = \"\".join(rmf['body'])\n",
    "summaries_bs = \"\".join(bs['body'])\n",
    "summaries_f = \"\".join(f['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n",
    "ngrams_summaries_sff = vect.build_analyzer()(summaries_sff)\n",
    "ngrams_summaries_rmf = vect.build_analyzer()(summaries_rmf)\n",
    "ngrams_summaries_bs = vect.build_analyzer()(summaries_bs)\n",
    "ngrams_summaries_f = vect.build_analyzer()(summaries_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hock seng', 579),\n",
       " (u'shook head', 470),\n",
       " (u'lieutenant awn', 397),\n",
       " (u'old man', 302),\n",
       " (u'long time', 266),\n",
       " (u'bene gesserit', 265),\n",
       " (u'princess nell', 242),\n",
       " (u'white shirts', 241),\n",
       " (u'years ago', 232),\n",
       " (u'judge fang', 210),\n",
       " (u'anaander mianaai', 201),\n",
       " (u'hiro says', 187),\n",
       " (u'muad dib', 181),\n",
       " (u'feyd rautha', 179),\n",
       " (u'thousand years', 171),\n",
       " (u'far away', 160),\n",
       " (u'reverend mother', 147),\n",
       " (u'pham nuwen', 147),\n",
       " (u'hong kong', 140),\n",
       " (u'uncle enzo', 132)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sci-fi books over-use character names\n",
    "# also a long time ago, far far away\n",
    "Counter(ngrams_summaries_sfbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'sai ias', 295),\n",
       " (u'shook head', 163),\n",
       " (u'dr clef', 162),\n",
       " (u'hell ship', 140),\n",
       " (u'father marco', 104),\n",
       " (u'explorer 410', 96),\n",
       " (u'im sure', 93),\n",
       " (u'im sorry', 73),\n",
       " (u'big guy', 58),\n",
       " (u'bruce lucent', 57),\n",
       " (u'long time', 55),\n",
       " (u'dr simon', 53),\n",
       " (u'alice gavin', 50),\n",
       " (u'clockwork plague', 48),\n",
       " (u'years ago', 47),\n",
       " (u'death ship', 47),\n",
       " (u'henry archer', 45),\n",
       " (u'gavin alice', 45),\n",
       " (u'make sure', 44),\n",
       " (u'far away', 43)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sff).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 518),\n",
       " (u'im going', 361),\n",
       " (u'deep breath', 255),\n",
       " (u'im sorry', 245),\n",
       " (u'im sure', 201),\n",
       " (u'closed eyes', 191),\n",
       " (u'long time', 179),\n",
       " (u'blue eyes', 162),\n",
       " (u'youre going', 160),\n",
       " (u'took deep', 157),\n",
       " (u'took deep breath', 140),\n",
       " (u'shaking head', 130),\n",
       " (u'living room', 129),\n",
       " (u'open door', 119),\n",
       " (u'oh aye', 112),\n",
       " (u'eyes closed', 103),\n",
       " (u'turned away', 98),\n",
       " (u'yes sir', 96),\n",
       " (u'oh yes', 96),\n",
       " (u'ive seen', 95)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# romance titles have some classics --> deep breath, closed eyes, emotional gestures\n",
    "Counter(ngrams_summaries_rmbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'lord dearborne', 182),\n",
       " (u'mrs goodbody', 151),\n",
       " (u'shook head', 139),\n",
       " (u'new york', 83),\n",
       " (u'im sorry', 80),\n",
       " (u'maverick junction', 67),\n",
       " (u'lady catherine', 63),\n",
       " (u'im sure', 59),\n",
       " (u'parking lot', 50),\n",
       " (u'lord peterby', 49),\n",
       " (u'lord dearbornes', 48),\n",
       " (u'dr brent', 46),\n",
       " (u'closed eyes', 44),\n",
       " (u'lady peterby', 41),\n",
       " (u'oh god', 40),\n",
       " (u'night stand', 39),\n",
       " (u'lady anne', 38),\n",
       " (u'rolled eyes', 38),\n",
       " (u'wasnt sure', 36),\n",
       " (u'im gonna', 36)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_rmf).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "char_name_stop = stop_words.union(['awn','muad','dib','hock','seng','nell','pham','nuwen', 'enzo','anaander','mianaai',\n",
    "                                   'rautha','feyd','willoughby','brandon','john','dashwood','jessica','fang','hiro',\n",
    "                                  'steele','anastasia','rochester','jennings','ian','middleton','tara','jean','fairfax',\n",
    "                                  'mcgraw','finkle', 'dearborne','dearbornes','peterby','anne','alice','henry','simon',\n",
    "                                  'gavin','marco','bruce','catherine','nicky','brent'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,3), stop_words=char_name_stop)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "summaries_sff = \"\".join(sff['body'])\n",
    "summaries_rmf = \"\".join(rmf['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n",
    "ngrams_summaries_sff = vect.build_analyzer()(summaries_sff)\n",
    "ngrams_summaries_rmf = vect.build_analyzer()(summaries_rmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 470),\n",
       " (u'old man', 302),\n",
       " (u'long time', 266),\n",
       " (u'bene gesserit', 265),\n",
       " (u'white shirts', 241),\n",
       " (u'years ago', 232),\n",
       " (u'thousand years', 171),\n",
       " (u'far away', 160),\n",
       " (u'reverend mother', 147),\n",
       " (u'hong kong', 140),\n",
       " (u'shakes head', 124),\n",
       " (u'make sure', 124),\n",
       " (u'sjandra kei', 121),\n",
       " (u'lord radch', 120),\n",
       " (u'closed eyes', 119),\n",
       " (u'long ago', 119),\n",
       " (u'right hand', 116),\n",
       " (u'half hour', 115),\n",
       " (u'years old', 114),\n",
       " (u'im sure', 106)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sfbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'sai ias', 295),\n",
       " (u'shook head', 163),\n",
       " (u'dr clef', 162),\n",
       " (u'hell ship', 140),\n",
       " (u'explorer 410', 96),\n",
       " (u'im sure', 93),\n",
       " (u'im sorry', 73),\n",
       " (u'big guy', 58),\n",
       " (u'long time', 55),\n",
       " (u'clockwork plague', 48),\n",
       " (u'years ago', 47),\n",
       " (u'death ship', 47),\n",
       " (u'make sure', 44),\n",
       " (u'far away', 43),\n",
       " (u'impossible cube', 43),\n",
       " (u'dar frowned', 41),\n",
       " (u'im going', 41),\n",
       " (u'ive got', 40),\n",
       " (u'dar nodded', 40),\n",
       " (u'young man', 40)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sff).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 518),\n",
       " (u'im going', 361),\n",
       " (u'deep breath', 255),\n",
       " (u'im sorry', 245),\n",
       " (u'im sure', 201),\n",
       " (u'closed eyes', 191),\n",
       " (u'long time', 179),\n",
       " (u'blue eyes', 162),\n",
       " (u'youre going', 160),\n",
       " (u'took deep', 157),\n",
       " (u'took deep breath', 140),\n",
       " (u'shaking head', 130),\n",
       " (u'living room', 129),\n",
       " (u'open door', 119),\n",
       " (u'oh aye', 112),\n",
       " (u'eyes closed', 103),\n",
       " (u'turned away', 98),\n",
       " (u'yes sir', 96),\n",
       " (u'oh yes', 96),\n",
       " (u'ive seen', 95),\n",
       " (u'young man', 95),\n",
       " (u'im glad', 95),\n",
       " (u'ye ken', 94),\n",
       " (u'new york', 92),\n",
       " (u'little bit', 91),\n",
       " (u'gray eyes', 88),\n",
       " (u'eyes fixed', 87),\n",
       " (u'tell ye', 85),\n",
       " (u'cleared throat', 84),\n",
       " (u'ive got', 81)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is funnier\n",
    "Counter(ngrams_summaries_rmbs).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'mrs goodbody', 151),\n",
       " (u'shook head', 139),\n",
       " (u'new york', 83),\n",
       " (u'im sorry', 80),\n",
       " (u'maverick junction', 67),\n",
       " (u'lady catherine', 63),\n",
       " (u'im sure', 59),\n",
       " (u'parking lot', 50),\n",
       " (u'dr brent', 46),\n",
       " (u'closed eyes', 44),\n",
       " (u'oh god', 40),\n",
       " (u'night stand', 39),\n",
       " (u'rolled eyes', 38),\n",
       " (u'wasnt sure', 36),\n",
       " (u'im gonna', 36),\n",
       " (u'opened door', 35),\n",
       " (u'im going', 35),\n",
       " (u'mr little', 35),\n",
       " (u'youre right', 33),\n",
       " (u'sacre bleu', 31),\n",
       " (u'living room', 30),\n",
       " (u'ive got', 30),\n",
       " (u'uncle nicky', 29),\n",
       " (u'held hand', 29),\n",
       " (u'lone tree', 29),\n",
       " (u'ice cream', 28),\n",
       " (u'oh yeah', 27),\n",
       " (u'make sure', 27),\n",
       " (u'youre going', 26),\n",
       " (u'high school', 25)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_rmf).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
