{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter, defaultdict\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epub_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_epub_working/\"\n",
    "txt_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_txt/\"\n",
    "path = '/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/'\n",
    "folders = ['sci-fi_top','sci-fi_flop','romance_top','romance_flop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load profanity file\n",
    "curses = pd.read_csv(path + 'other/profanity.csv')\n",
    "curses.drop('Unnamed: 1', inplace=True, axis=1)\n",
    "bad_words = curses.word.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load character name file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to extract text from epub\n",
    "\n",
    "def convert_epub_to_text(epub_path, epub_file, txt_path):\n",
    "\n",
    "    clean_text = ''\n",
    "    text_name = epub_file.replace(' ','_')[:-4]+'txt'\n",
    "    \n",
    "    # extract text from epub\n",
    "    text = textract.process(epub_path+epub_file,encoding='utf_8')\n",
    "    \n",
    "    # trip out the unicode and return characters\n",
    "    # still working on the \\ characters\n",
    "    #for i in text.split(' '):\n",
    "    clean_text = text.decode('ascii', 'ignore').replace('\\n',' ')\n",
    "    \n",
    "    # save as text file\n",
    "    text_file = open(txt_path+text_name, 'w')\n",
    "    text_file.write(clean_text)\n",
    "    text_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colour_had_all_been_packed_away-libby_oneill.epub  failed\n",
      "good_breeding-jl_merrow.epub  failed\n",
      "iastron-james_dunn.epub  failed\n",
      "star_diary-andreas_ingo.epub  failed\n",
      "tales_from_inter _space_freight_services-duane_smith.epub  failed\n",
      "the_edge_of_eternity-mark_holzclaw.epub  failed\n"
     ]
    }
   ],
   "source": [
    "# loop through epub files in directory, extract text, save text file in new folder\n",
    "for epub in os.listdir(epub_path):\n",
    "    try:\n",
    "        convert_epub_to_text(epub_path, epub, txt_path)\n",
    "    except:\n",
    "        print epub, \" failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load txt files into dataframe, \n",
    "# give each entry a best_selling 1/0 indicator and a sci_fi 1/0 (0=romance) indicator\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    if folder[-3:]=='top':\n",
    "        bs = 1\n",
    "    else:\n",
    "        bs = 0\n",
    "    if folder[:3]=='sci':\n",
    "        sf = 1\n",
    "    else:\n",
    "        sf = 0\n",
    "        \n",
    "    for text_file in os.listdir(path+folder+'/'):\n",
    "        full_path = path + folder + '/' + text_file\n",
    "        if text_file.endswith((\".txt\")):\n",
    "            text  = open(full_path, 'r').read()\n",
    "            temp = pd.DataFrame({\n",
    "                    'best_seller': bs,\n",
    "                    'sci_fi': sf,\n",
    "                    'title': text_file[:-4].replace('_',' ').replace('-',' - '),\n",
    "                    'body': text.decode('ascii', 'ignore').replace('\\n',' ').replace('\\r','')}, \n",
    "                                index=[0])\n",
    "            df = pd.concat([df, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_seller</th>\n",
       "      <th>body</th>\n",
       "      <th>sci_fi</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Prologue  The sun is always just about to ris...</td>\n",
       "      <td>1</td>\n",
       "      <td>2312 - kim stanley robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How to explain? How to describe? Even the omni...</td>\n",
       "      <td>1</td>\n",
       "      <td>a fire upon the deep - vernor vinge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_seller                                               body  sci_fi  \\\n",
       "0            1   Prologue  The sun is always just about to ris...       1   \n",
       "1            1  How to explain? How to describe? Even the omni...       1   \n",
       "\n",
       "                                 title  \n",
       "0          2312 - kim stanley robinson  \n",
       "1  a fire upon the deep - vernor vinge  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# house cleaning\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fire upon the deep - vernor vinge  :  appendix\n",
      "dune - frank herbert  :  appendix\n",
      "rainbows end - vernor vinge  :  table of contents\n",
      "rainbows end - vernor vinge  :  appendix\n",
      "rainbows end - vernor vinge  :  copyright\n",
      "the algebraist - iain m banks  :  acknowledgements\n",
      "the algebraist - iain m banks  :  appendix\n",
      "atlanta nights - travis tea  :  appendix\n",
      "red planet pioneer - vincent tibbetts  :  copyright\n",
      "fifty shades of grey - e l james  :  appendix\n",
      "grey - e l james  :  appendix\n",
      "outlander voyager - diana galbaldon  :  appendix\n",
      "the rosie project - graeme simsion  :  table of contents\n",
      "harder - brenda cooper  :  copyright\n"
     ]
    }
   ],
   "source": [
    "# check for front and back matter in body\n",
    "# i have cleaned up the files by using this alert\n",
    "# remaining issues are intentional usage of terms in the text\n",
    "\n",
    "check_words = ['acknowledgements','table of contents','about the author', 'appendix', \n",
    "               'copyright','isbn','by this author']\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    for word in check_words:\n",
    "        if word in df.iloc[i,1].lower():\n",
    "            print df.ix[i,3], ' : ', word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function bank\n",
    "\n",
    "def avg_sentence_len(text):\n",
    "    word_counts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sent_detect = PunktSentenceTokenizer()\n",
    "    sentences = sent_detect.sentences_from_text(text)\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        word_counts.append(len(words))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)  \n",
    "    return avg_word_count\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "def get_token_words(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    return words\n",
    "\n",
    "def word_count(text):\n",
    "    words = get_token_words(text)\n",
    "    return len(words)\n",
    "\n",
    "def avg_word_len(text):\n",
    "    letter_counts = []\n",
    "    words = get_token_words(text)\n",
    "    for word in words:\n",
    "        letter_counts.append(len(word))\n",
    "    avg_word_len = sum(letter_counts)/len(letter_counts)\n",
    "    return avg_word_len\n",
    "\n",
    "def profanity_counter(text):\n",
    "    i=0\n",
    "    words = get_token_words(text)\n",
    "    for word in words:\n",
    "        if word in bad_words:\n",
    "            i+=1       \n",
    "    return i\n",
    "\n",
    "def lex_div(text):\n",
    "    words = get_token_words(text)\n",
    "    lexical_diversity = 1.0 * len(set(words)) / len(words)\n",
    "    return lexical_diversity\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "def to_blob(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob\n",
    "\n",
    "def assign_polarity(text):\n",
    "    blob = to_blob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def assign_subjectivity(text):\n",
    "    blob = to_blob(text)\n",
    "    return blob.sentiment.subjectivity\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "def parse_pos(df,field):\n",
    "    for i in range(0,len(df)):\n",
    "        blob = TextBlob(df.ix[i,field])\n",
    "        tags = blob.tags\n",
    "        df_tags = pd.DataFrame(tags)\n",
    "        df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "        for x in range(0,len(df_tags)):\n",
    "                df.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "        df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# blobs = df['body'].map(to_blob)\n",
    "# words = df['body'].map(get_token_words)\n",
    "\n",
    "# create metrics\n",
    "\n",
    "df['avg_sent_len'] = df['body'].map(avg_sentence_len)\n",
    "df['word_count'] = df['body'].map(word_count)\n",
    "df['avg_word_len'] = df['body'].map(avg_word_len)\n",
    "df['lex_diversity'] = df['body'].map(lex_div)\n",
    "df['polarity'] = df['body'].map(assign_polarity)\n",
    "df['subjectivity'] = df['body'].map(assign_subjectivity)\n",
    "df['profanity'] = df['body'].map(profanity_counter)\n",
    "df['profane'] = 1. * df['profanity']/df['word_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parse_pos(df,'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(0,len(df)):\n",
    "#     blob = TextBlob(df.ix[i,'body'])\n",
    "#     tags = blob.tags\n",
    "#     df_tags = pd.DataFrame(tags)\n",
    "#     df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "#     for x in range(0,len(df_tags)):\n",
    "#             df.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "#     df.fillna(0,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'best_seller', u'body', u'sci_fi', u'title', u'avg_sent_len',\n",
       "       u'word_count', u'avg_word_len', u'lex_diversity', u'polarity',\n",
       "       u'subjectivity', u'profanity', u'profane', u'conj_coord', u'number',\n",
       "       u'determiner', u'exist_there', u'foreign_word', u'conj_sub_prep',\n",
       "       u'adj', u'adj_compare', u'adj_sup', u'verb_aux', u'noun', u'noun_prop',\n",
       "       u'noun_prop_pural', u'noun_plural', u'predeterm', u'pronoun_pers',\n",
       "       u'pronoun_poss', u'adv', u'adv_compare', u'adv_sup', u'adv_part',\n",
       "       u'inf_to', u'interject', u'verb_base', u'verb_past', u'verb_ger',\n",
       "       u'verb_pp', u'verb_sing_pres', u'verb_3rd_sing_pres', u'wh_determ',\n",
       "       u'wh_pronoun', u'wh_poss', u'wh_adv', u'poss_ending', u'symbol',\n",
       "       u'list_marker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'CC':'conj_coord','CD':'number', 'DT':'determiner', 'EX':'exist_there',\n",
    "                  'FW':'foreign_word',  'IN':'conj_sub_prep','JJ':'adj','JJR':'adj_compare',\n",
    "                 'JJS':'adj_sup','MD':'verb_aux',  'NN':'noun','NNP':'noun_prop',\n",
    "                'NNPS':'noun_prop_pural',  'NNS':'noun_plural', 'PDT':'predeterm','PRP':'pronoun_pers',\n",
    "                'PRP$':'pronoun_poss',  'RB':'adv','RBR':'adv_compare','RBS':'adv_sup',\n",
    "                  'RP':'adv_part', 'TO':'inf_to',  'UH':'interject','VB':'verb_base',\n",
    "                 'VBD':'verb_past','VBG':'verb_ger','VBN':'verb_pp','VBP':'verb_sing_pres',\n",
    "                 'VBZ':'verb_3rd_sing_pres','WDT':'wh_determ','WP':'wh_pronoun','WP$':'wh_poss',\n",
    "                 'WRB':'wh_adv','POS':'poss_ending','SYM':'symbol','LS':'list_marker'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path + 'df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = df[(df['best_seller']==1)]\n",
    "f = df[(df['best_seller']==0)]\n",
    "sfbs = df[((df['best_seller']==1) & (df['sci_fi']==1))]\n",
    "rmbs = df[((df['best_seller']==1) & (df['sci_fi']==0))]\n",
    "sff = df[((df['best_seller']==0) & (df['sci_fi']==1))]\n",
    "rmf = df[((df['best_seller']==0) & (df['sci_fi']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sfbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sff.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rmbs.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rmf.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union(['232'])\n",
    "char_name_stop = stop_words.union(['awn','muad','dib','hock','seng','nell','pham','nuwen', 'enzo','anaander','mianaai',\n",
    "                                   'rautha','feyd','willoughby','brandon','john','dashwood','jessica','fang','hiro',\n",
    "                                  'steele','anastasia','rochester','jennings','ian','middleton','tara','jean','fairfax',\n",
    "                                  'mcgraw','finkle', 'dearborne','dearbornes','peterby','anne','alice','henry','simon',\n",
    "                                  'gavin','marco','bruce','catherine','nicky','brent','reverend','bene','gesserit','clef',\n",
    "                                  'radch','kei',\"sjandra\",'goodbody','paul','robert','horza','ender','ruby','travis','dar',\n",
    "                                  'miller','holden','travis','leto','mike','anna','justus','swan','jeff','onor','stilgar',\n",
    "                                  'sam','jamie','sasha','riley','nerezza','christian','grey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial feature review to see anything interesting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), stop_words=char_name_stop)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "summaries_sff = \"\".join(sff['body'])\n",
    "summaries_rmf = \"\".join(rmf['body'])\n",
    "summaries_bs = \"\".join(bs['body'])\n",
    "summaries_f = \"\".join(f['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n",
    "ngrams_summaries_sff = vect.build_analyzer()(summaries_sff)\n",
    "ngrams_summaries_rmf = vect.build_analyzer()(summaries_rmf)\n",
    "ngrams_summaries_bs = vect.build_analyzer()(summaries_bs)\n",
    "ngrams_summaries_f = vect.build_analyzer()(summaries_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'said', 27773),\n",
       " (u'like', 14232),\n",
       " (u'just', 11302),\n",
       " (u'time', 9944),\n",
       " (u'know', 9796),\n",
       " (u'way', 7608),\n",
       " (u'did', 7151),\n",
       " (u'im', 6831),\n",
       " (u'eyes', 6654),\n",
       " (u'think', 6555),\n",
       " (u'hand', 6430),\n",
       " (u'head', 6340),\n",
       " (u'dont', 6281),\n",
       " (u'thought', 6238),\n",
       " (u'going', 6090),\n",
       " (u'looked', 6015),\n",
       " (u'right', 5908),\n",
       " (u'didnt', 5700),\n",
       " (u'away', 5677),\n",
       " (u'little', 5672)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_bs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'said', 10446),\n",
       " (u'like', 9050),\n",
       " (u'just', 7450),\n",
       " (u'know', 6576),\n",
       " (u'im', 6150),\n",
       " (u'dont', 5846),\n",
       " (u'time', 5504),\n",
       " (u'way', 4519),\n",
       " (u'didnt', 4504),\n",
       " (u'did', 4246),\n",
       " (u'eyes', 4099),\n",
       " (u'think', 3987),\n",
       " (u'head', 3967),\n",
       " (u'right', 3858),\n",
       " (u'going', 3822),\n",
       " (u'looked', 3769),\n",
       " (u'want', 3730),\n",
       " (u'hand', 3640),\n",
       " (u'away', 3450),\n",
       " (u'look', 3264)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_f).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'said', 16618),\n",
       " (u'like', 8113),\n",
       " (u'just', 6633),\n",
       " (u'time', 5624),\n",
       " (u'know', 4966),\n",
       " (u'way', 4626),\n",
       " (u'people', 4198),\n",
       " (u'did', 3921),\n",
       " (u'looked', 3706),\n",
       " (u'little', 3406),\n",
       " (u'thought', 3386),\n",
       " (u'man', 3173),\n",
       " (u'think', 3159),\n",
       " (u'long', 3156),\n",
       " (u'away', 3147),\n",
       " (u'right', 3125),\n",
       " (u'going', 2930),\n",
       " (u'old', 2783),\n",
       " (u'got', 2766),\n",
       " (u'head', 2757)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sci-fi books over-use character names\n",
    "# also a long time ago, far far away\n",
    "Counter(ngrams_summaries_sfbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'said', 7156),\n",
       " (u'like', 4863),\n",
       " (u'just', 4056),\n",
       " (u'know', 3395),\n",
       " (u'time', 3181),\n",
       " (u'way', 2772),\n",
       " (u'dont', 2669),\n",
       " (u'did', 2428),\n",
       " (u'looked', 2367),\n",
       " (u'people', 2236),\n",
       " (u'im', 2205),\n",
       " (u'didnt', 2116),\n",
       " (u'think', 2110),\n",
       " (u'going', 2012),\n",
       " (u'right', 1962),\n",
       " (u'head', 1927),\n",
       " (u'eyes', 1868),\n",
       " (u'away', 1866),\n",
       " (u'little', 1825),\n",
       " (u'hand', 1758)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sff).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'said', 11155),\n",
       " (u'like', 6119),\n",
       " (u'im', 5455),\n",
       " (u'know', 4830),\n",
       " (u'just', 4669),\n",
       " (u'dont', 4507),\n",
       " (u'time', 4320),\n",
       " (u'eyes', 4170),\n",
       " (u'hand', 3890),\n",
       " (u'didnt', 3754),\n",
       " (u'head', 3583),\n",
       " (u'want', 3551),\n",
       " (u'think', 3396),\n",
       " (u'did', 3230),\n",
       " (u'going', 3160),\n",
       " (u'face', 3118),\n",
       " (u'good', 3006),\n",
       " (u'way', 2982),\n",
       " (u'youre', 2914),\n",
       " (u'thought', 2852)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# romance titles have some classics --> deep breath, closed eyes, emotional gestures\n",
    "Counter(ngrams_summaries_rmbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'like', 4187),\n",
       " (u'im', 3945),\n",
       " (u'just', 3394),\n",
       " (u'said', 3290),\n",
       " (u'know', 3181),\n",
       " (u'dont', 3177),\n",
       " (u'didnt', 2388),\n",
       " (u'want', 2349),\n",
       " (u'time', 2323),\n",
       " (u'eyes', 2231),\n",
       " (u'head', 2040),\n",
       " (u'right', 1896),\n",
       " (u'hand', 1882),\n",
       " (u'think', 1877),\n",
       " (u'did', 1818),\n",
       " (u'going', 1810),\n",
       " (u'way', 1747),\n",
       " (u'good', 1653),\n",
       " (u'face', 1622),\n",
       " (u'shed', 1589)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_rmf).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,3), stop_words=char_name_stop)\n",
    "\n",
    "summaries_sfbs = \"\".join(sfbs['body'])\n",
    "summaries_rmbs = \"\".join(rmbs['body'])\n",
    "summaries_sff = \"\".join(sff['body'])\n",
    "summaries_rmf = \"\".join(rmf['body'])\n",
    "\n",
    "ngrams_summaries_sfbs = vect.build_analyzer()(summaries_sfbs)\n",
    "ngrams_summaries_rmbs = vect.build_analyzer()(summaries_rmbs)\n",
    "ngrams_summaries_sff = vect.build_analyzer()(summaries_sff)\n",
    "ngrams_summaries_rmf = vect.build_analyzer()(summaries_rmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 456),\n",
       " (u'long time', 314),\n",
       " (u'old man', 311),\n",
       " (u'years ago', 276),\n",
       " (u'white shirts', 241),\n",
       " (u'thousand years', 206),\n",
       " (u'far away', 184),\n",
       " (u'cloud ark', 183),\n",
       " (u'hong kong', 140),\n",
       " (u'im sure', 136),\n",
       " (u'half hour', 132),\n",
       " (u'im going', 128),\n",
       " (u'make sure', 126),\n",
       " (u'long ago', 126),\n",
       " (u'years old', 124),\n",
       " (u'shakes head', 124),\n",
       " (u'right hand', 122),\n",
       " (u'closed eyes', 122),\n",
       " (u'time time', 119),\n",
       " (u'half dozen', 116)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sfbs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 317),\n",
       " (u'sai ias', 295),\n",
       " (u'im sorry', 192),\n",
       " (u'im sure', 184),\n",
       " (u'im going', 156),\n",
       " (u'long time', 145),\n",
       " (u'make sure', 137),\n",
       " (u'deep breath', 115),\n",
       " (u'hell ship', 114),\n",
       " (u'old man', 106),\n",
       " (u'years ago', 105),\n",
       " (u'took deep', 102),\n",
       " (u'purple man', 100),\n",
       " (u'youre going', 98),\n",
       " (u'closed eyes', 97),\n",
       " (u'explorer 410', 96),\n",
       " (u'took deep breath', 93),\n",
       " (u'far away', 92),\n",
       " (u'diamond deep', 83),\n",
       " (u'purple men', 80)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_sff).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 694),\n",
       " (u'im going', 601),\n",
       " (u'im sorry', 355),\n",
       " (u'im sure', 296),\n",
       " (u'deep breath', 276),\n",
       " (u'long time', 240),\n",
       " (u'youre going', 240),\n",
       " (u'closed eyes', 218),\n",
       " (u'blue eyes', 185),\n",
       " (u'new york', 177),\n",
       " (u'took deep', 174),\n",
       " (u'wasnt sure', 162),\n",
       " (u'living room', 161),\n",
       " (u'took deep breath', 155),\n",
       " (u'shaking head', 145),\n",
       " (u'ive got', 143),\n",
       " (u'open door', 132),\n",
       " (u'im glad', 125),\n",
       " (u'oh god', 123),\n",
       " (u'little bit', 123),\n",
       " (u'ive seen', 120),\n",
       " (u'years ago', 115),\n",
       " (u'young man', 114),\n",
       " (u'make sure', 114),\n",
       " (u'wasnt going', 113),\n",
       " (u'eyes closed', 113),\n",
       " (u'oh aye', 112),\n",
       " (u'youve got', 110),\n",
       " (u'oh yes', 108),\n",
       " (u'turned away', 107)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is funnier\n",
    "Counter(ngrams_summaries_rmbs).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'shook head', 325),\n",
       " (u'im sorry', 303),\n",
       " (u'im going', 288),\n",
       " (u'im sure', 250),\n",
       " (u'mrs goodbody', 151),\n",
       " (u'living room', 147),\n",
       " (u'ive got', 141),\n",
       " (u'closed eyes', 138),\n",
       " (u'new york', 137),\n",
       " (u'parking lot', 133),\n",
       " (u'high school', 125),\n",
       " (u'im just', 117),\n",
       " (u'make sure', 112),\n",
       " (u'wasnt sure', 110),\n",
       " (u'long time', 107),\n",
       " (u'opened door', 102),\n",
       " (u'years ago', 100),\n",
       " (u'oh god', 98),\n",
       " (u'youre going', 97),\n",
       " (u'deep breath', 92),\n",
       " (u'shakes head', 92),\n",
       " (u'youve got', 88),\n",
       " (u'think im', 86),\n",
       " (u'im glad', 85),\n",
       " (u'rolled eyes', 81),\n",
       " (u'youre right', 77),\n",
       " (u'held hand', 75),\n",
       " (u'just want', 73),\n",
       " (u'pulled away', 73),\n",
       " (u'ive seen', 71)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ngrams_summaries_rmf).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=char_name_stop)\n",
    "X = vect.fit_transform(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'0.005*time + 0.004*people + 0.004*just'),\n",
       " (1, u'0.006*just + 0.004*looked + 0.004*time'),\n",
       " (2, u'0.006*just + 0.005*says + 0.004*time'),\n",
       " (3, u'0.004*thought + 0.004*man + 0.003*way'),\n",
       " (4, u'0.005*just + 0.005*time + 0.003*way')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_sf1 = CountVectorizer(stop_words=char_name_stop)\n",
    "X_sf1 = vect_sf1.fit_transform(sfbs['body'])\n",
    "vocab_sf1 = {v: k for k, v in vect_sf1.vocabulary_.iteritems()}\n",
    "\n",
    "lda_sf1 = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X_sf1, documents_columns=False),\n",
    "    num_topics  =  5,\n",
    "    passes      =  5,\n",
    "    id2word     =  vocab_sf1\n",
    ")\n",
    "lda_sf1.print_topics(num_topics=5, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'0.005*time + 0.004*just + 0.004*way'),\n",
       " (1, u'0.006*people + 0.005*just + 0.005*time'),\n",
       " (2, u'0.007*just + 0.005*time + 0.004*way'),\n",
       " (3, u'0.007*just + 0.005*im + 0.005*time'),\n",
       " (4, u'0.004*just + 0.004*looked + 0.004*time')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_sf0 = CountVectorizer(stop_words=char_name_stop)\n",
    "X_sf0 = vect_sf0.fit_transform(sff['body'])\n",
    "vocab_sf0 = {v: k for k, v in vect_sf0.vocabulary_.iteritems()}\n",
    "\n",
    "lda_sf0 = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X_sf0, documents_columns=False),\n",
    "    num_topics  =  5,\n",
    "    passes      =  5,\n",
    "    id2word     =  vocab_sf0\n",
    ")\n",
    "lda_sf0.print_topics(num_topics=5, num_words=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'0.006*time + 0.005*rosie + 0.005*im'),\n",
       " (1, u'0.008*im + 0.007*just + 0.006*time'),\n",
       " (2, u'0.009*ye + 0.004*hand + 0.004*head'),\n",
       " (3, u'0.006*hand + 0.006*eyes + 0.005*ye'),\n",
       " (4, u'0.010*im + 0.007*want + 0.006*eyes')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_rm1 = CountVectorizer(stop_words=char_name_stop)\n",
    "X_rm1 = vect_rm1.fit_transform(rmbs['body'])\n",
    "vocab_rm1 = {v: k for k, v in vect_rm1.vocabulary_.iteritems()}\n",
    "\n",
    "lda_rm1 = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X_rm1, documents_columns=False),\n",
    "    num_topics  =  5,\n",
    "    passes      =  5,\n",
    "    id2word     =  vocab_rm1\n",
    ")\n",
    "lda_rm1.print_topics(num_topics=5, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'0.008*beltane + 0.007*expressions + 0.005*delightfulbut'),\n",
       " (1, u'0.009*moths + 0.009*beenbut + 0.007*countermoves'),\n",
       " (2, u'0.010*electronic + 0.007*expressions + 0.006*particularly'),\n",
       " (3, u'0.009*electronic + 0.005*expressions + 0.005*leastout'),\n",
       " (4, u'0.007*expressions + 0.006*electronic + 0.005*nerezza')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_rm0 = CountVectorizer(stop_words=char_name_stop)\n",
    "X_rm0 = vect_rm0.fit_transform(rmf['body'])\n",
    "vocab_rm0 = {v: k for k, v in vect_rm1.vocabulary_.iteritems()}\n",
    "\n",
    "lda_rm0 = models.LdaModel(\n",
    "    matutils.Sparse2Corpus(X_rm0, documents_columns=False),\n",
    "    num_topics  =  5,\n",
    "    passes      =  5,\n",
    "    id2word     =  vocab_rm0\n",
    ")\n",
    "lda_rm0.print_topics(num_topics=5, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
