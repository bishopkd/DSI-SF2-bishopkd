{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah = \"Frozen, bruised, and bloody as she was, I knew her. Her name was Seivarden Vendaai! And a long time ago she had been one of my officers? a young lieutenant, eventually promoted to her own command, another ship. I had thought her a thousand years dead, but she was, undeniably, here. I crouched down and felt for a pulse, for the faintest stir of breath.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah2 = \"I am so very happy. I am thrilled. But then I hate the cruelty of death, destruction, decomposition. And now I feel sad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  Frozen, bruised, and bloody as she was, I knew...\n",
       "1  I am so very happy. I am thrilled. But then I ...\n",
       "2  Frozen, bruised, and bloody as she was, I knew...\n",
       "3  I am so very happy. I am thrilled. But then I ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geez = {'body':[blah,blah2,blah,blah2]}\n",
    "test = pd.DataFrame(geez)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(test)):\n",
    "    blob = TextBlob(test.ix[i,0])\n",
    "    tags = blob.tags\n",
    "    df_tags = pd.DataFrame(tags)\n",
    "    df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "    for x in range(0,len(df_tags)):\n",
    "            test.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "            \n",
    "test.fillna(0,inplace=True)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>IN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJS</th>\n",
       "      <th>NN</th>\n",
       "      <th>NNP</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PRP</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>RB</th>\n",
       "      <th>TO</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   CC   CD   DT   IN   JJ  \\\n",
       "0  Frozen, bruised, and bloody as she was, I knew...  4.0  2.0  6.0  6.0  4.0   \n",
       "1  I am so very happy. I am thrilled. But then I ...  2.0  0.0  1.0  1.0  2.0   \n",
       "2  Frozen, bruised, and bloody as she was, I knew...  4.0  2.0  6.0  6.0  4.0   \n",
       "3  I am so very happy. I am thrilled. But then I ...  2.0  0.0  1.0  1.0  2.0   \n",
       "\n",
       "   JJS   NN  NNP  NNS  PRP  PRP$   RB   TO  VBD  VBN  VBP  \n",
       "0  1.0  9.0  3.0  2.0  7.0   4.0  4.0  1.0  9.0  3.0  0.0  \n",
       "1  0.0  4.0  0.0  0.0  4.0   0.0  4.0  0.0  0.0  1.0  4.0  \n",
       "2  1.0  9.0  3.0  2.0  7.0   4.0  4.0  1.0  9.0  3.0  0.0  \n",
       "3  0.0  4.0  0.0  0.0  4.0   0.0  4.0  0.0  0.0  1.0  4.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PRP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RB</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VBP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  0\n",
       "0   CC  2\n",
       "1   DT  1\n",
       "2   IN  1\n",
       "3   JJ  2\n",
       "4   NN  4\n",
       "5  PRP  4\n",
       "6   RB  4\n",
       "7  VBN  1\n",
       "8  VBP  4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(blah2)\n",
    "tags = blob.tags\n",
    "df_tags = pd.DataFrame(tags)\n",
    "df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIN\n",
      "WIN\n"
     ]
    }
   ],
   "source": [
    "from pattern.web    import Twitter\n",
    "from pattern.en     import tag\n",
    "from pattern.vector import KNN, count\n",
    "\n",
    "twitter, knn = Twitter(), KNN()\n",
    "\n",
    "for i in range(1, 3):\n",
    "    for tweet in twitter.search('#win OR #fail', start=i, count=100):\n",
    "        s = tweet.text.lower()\n",
    "        p = '#win' in s and 'WIN' or 'FAIL'\n",
    "        v = tag(s)\n",
    "        v = [word for word, pos in v if pos == 'JJ'] # JJ = adjective\n",
    "        v = count(v) # {'sweet': 1}\n",
    "        if v:\n",
    "            knn.train(v, type=p)\n",
    "\n",
    "print knn.classify('sweet potato burger')\n",
    "print knn.classify('stupid autocorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "29\tverb.body\tverbs of grooming, dressing and bodily care\n",
    "30\tverb.change\tverbs of size, temperature change, intensifying, etc.\n",
    "31\tverb.cognition\tverbs of thinking, judging, analyzing, doubting\n",
    "32\tverb.communication\tverbs of telling, asking, ordering, singing\n",
    "33\tverb.competition\tverbs of fighting, athletic activities\n",
    "34\tverb.consumption\tverbs of eating and drinking\n",
    "35\tverb.contact\tverbs of touching, hitting, tying, digging\n",
    "36\tverb.creation\tverbs of sewing, baking, painting, performing\n",
    "37\tverb.emotion\tverbs of feeling\n",
    "38\tverb.motion\tverbs of walking, flying, swimming\n",
    "39\tverb.perception\tverbs of seeing, hearing, feeling\n",
    "40\tverb.possession\tverbs of buying, selling, owning\n",
    "41\tverb.social\tverbs of political and social activities and events\n",
    "42\tverb.stative\tverbs of being, having, spatial relations\n",
    "43\tverb.weather\tverbs of raining, snowing, thawing, thundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#words = tokenizer.tokenize(text)\n",
    "# for i in range(1, 3):\n",
    "#     for tweet in twitter.search('#win OR #fail', start=i, count=100):\n",
    "lower_blah = blah.lower()\n",
    "        #p = '#win' in s and 'WIN' or 'FAIL'\n",
    "v = tag(lower_blah)\n",
    "v = [word for word, pos in v if pos[:2] == 'VB'] # VB* = verb\n",
    "#v = count(v) \n",
    "    #if v:\n",
    "            #knn.train(v, type=p)\n",
    "\n",
    "#print knn.classify('sweet potato burger')\n",
    "#print knn.classify('stupid autocorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tst = pd.read_csv('/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs2=[]\n",
    "\n",
    "words = nltk.word_tokenize(blah)\n",
    "for word,pos in nltk.pos_tag(words):\n",
    "    if pos[:2] == 'VB': \n",
    "        verbs2.append(word+'-'+pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs3=[]\n",
    "\n",
    "words = nltk.word_tokenize(blah)\n",
    "for word,pos in nltk.pos_tag(words):\n",
    "    if pos[:2] == 'VB': \n",
    "        verbs3.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_set = list(set(verbs2))\n",
    "len(verb_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bruised',\n",
       " 'was',\n",
       " 'knew',\n",
       " 'was',\n",
       " 'had',\n",
       " 'been',\n",
       " 'promoted',\n",
       " 'had',\n",
       " 'thought',\n",
       " 'was',\n",
       " 'crouched',\n",
       " 'felt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruised verb.contact\n",
      "bruised verb.emotion\n",
      "bruised verb.change\n",
      "bruised verb.change\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.social\n",
      "was verb.stative\n",
      "was verb.possession\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.contact\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "knew verb.cognition\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.social\n",
      "was verb.stative\n",
      "was verb.possession\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "had verb.possession\n",
      "had verb.stative\n",
      "had verb.perception\n",
      "had verb.possession\n",
      "had verb.change\n",
      "had verb.consumption\n",
      "had verb.social\n",
      "had verb.creation\n",
      "had verb.possession\n",
      "had verb.stative\n",
      "had verb.change\n",
      "had verb.body\n",
      "had verb.communication\n",
      "had verb.possession\n",
      "had verb.possession\n",
      "had verb.body\n",
      "had verb.possession\n",
      "had verb.body\n",
      "had verb.contact\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.social\n",
      "been verb.stative\n",
      "been verb.possession\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "been verb.stative\n",
      "promoted verb.social\n",
      "promoted verb.social\n",
      "promoted verb.communication\n",
      "promoted verb.competition\n",
      "promoted verb.competition\n",
      "had verb.possession\n",
      "had verb.stative\n",
      "had verb.perception\n",
      "had verb.possession\n",
      "had verb.change\n",
      "had verb.consumption\n",
      "had verb.social\n",
      "had verb.creation\n",
      "had verb.possession\n",
      "had verb.stative\n",
      "had verb.change\n",
      "had verb.body\n",
      "had verb.communication\n",
      "had verb.possession\n",
      "had verb.possession\n",
      "had verb.body\n",
      "had verb.possession\n",
      "had verb.body\n",
      "had verb.contact\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.creation\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.cognition\n",
      "thought verb.change\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.social\n",
      "was verb.stative\n",
      "was verb.possession\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "was verb.stative\n",
      "crouched verb.motion\n",
      "crouched verb.contact\n",
      "felt verb.contact\n",
      "felt verb.contact\n",
      "felt verb.change\n",
      "felt verb.emotion\n",
      "felt verb.cognition\n",
      "felt verb.perception\n",
      "felt verb.body\n",
      "felt verb.cognition\n",
      "felt verb.perception\n",
      "felt verb.stative\n",
      "felt verb.perception\n",
      "felt verb.contact\n",
      "felt verb.contact\n",
      "felt verb.possession\n",
      "felt verb.perception\n",
      "felt verb.contact\n"
     ]
    }
   ],
   "source": [
    "for i in verbs3:\n",
    "    for synset in wordnet.synsets(i,'v'):\n",
    "        print  i, synset.lexname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_pos(df,field):\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        \n",
    "        blob = TextBlob(df.ix[i,field])\n",
    "        tags = blob.tags\n",
    "        df_tags = pd.DataFrame(tags)\n",
    "        df_tags = df_tags.groupby([1]).count().reset_index() # create a count of POS tags \n",
    "        \n",
    "        verbs=[] # create verb-only list\n",
    "        for word,pos in tags:\n",
    "            if pos[:2] == 'VB': \n",
    "                verbs.append((word, wordnet.synsets(word,'v')[0].lexname())) # get lexical name for each verb\n",
    "        df_verbs = pd.DataFrame(verbs)\n",
    "        df_verbs = df_verbs.groupby([1]).count().reset_index() # create a count of verb subtype tags\n",
    "        del df_tags['index']\n",
    "                \n",
    "    df_tags = pd.concat([df_tags,df_verbs], axis=0)            # concat the pos df and the verb df\n",
    "    return df_tags\n",
    "    #for x in range(0,len(df_tags)):                            # add these as new columns to df           \n",
    "            #df_tst.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]   \n",
    "    #df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verb.cognition</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verb.contact</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verb.motion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>verb.possession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verb.social</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>verb.stative</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1  0\n",
       "0   verb.cognition  2\n",
       "1     verb.contact  2\n",
       "2      verb.motion  1\n",
       "3  verb.possession  2\n",
       "4      verb.social  1\n",
       "5     verb.stative  4"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs=[]\n",
    "blob = TextBlob(blah)\n",
    "tags = blob.tags\n",
    "for word,pos in tags:\n",
    "    if pos[:2] == 'VB':\n",
    "        verbs.append((word, wordnet.synsets(word,'v')[0].lexname()))\n",
    "vbdf = pd.DataFrame(verbs)\n",
    "    #for synset in wordnet.synsets(i,'v'):\n",
    "        #print  word, synset.lexname()\n",
    "        #verbs.append((word,synset.lexname()))\n",
    "df_verbs = pd.DataFrame(verbs)\n",
    "df_verbs = df_verbs.groupby([1]).count().reset_index()\n",
    "df_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vstems = []\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# for word in verb_set:\n",
    "#     stem = stemmer.stem(word)\n",
    "#     vstems.append(stem.lower())\n",
    "\n",
    "# vstems[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vstems = list(set(vstems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16536"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vstems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (path+\"stem.csv\",\"w\") as fp:\n",
    "    for line in vstems:\n",
    "        fp.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolv\n",
      "absorb\n",
      "absorb\n",
      "absorb\n",
      "absorb\n",
      "abstain\n",
      "abstain\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "test = ['absolved','absorb','absorbed','absorbing','absorbs','abstain','abstained']\n",
    "for word in test:\n",
    "    print stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# back up\n",
    "\n",
    "# def parse_pos(df,field):\n",
    "#     for i in range(0,len(df)):\n",
    "#         blob = TextBlob(df.ix[i,field])\n",
    "#         tags = blob.tags\n",
    "#         df_tags = pd.DataFrame(tags)\n",
    "#         df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "#         for x in range(0,len(df_tags)):\n",
    "#                 df.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "#         df.fillna(0,inplace=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
