{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_sentence_len(text):\n",
    "    word_counts = []\n",
    "    sentences = re.split(r'[.|!|?]', text)\n",
    "    for sentence in sentences[:-1]:\n",
    "        words = sentence.split()\n",
    "        word_counts.append(len(words))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)  \n",
    "    return avg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah = \"Frozen, bruised, and bloody as she was, I knew her. Her name was Seivarden Vendaai! And a long time ago she had been one of my officers? a young lieutenant, eventually promoted to her own command, another ship. I had thought her a thousand years dead, but she was, undeniably, here. I crouched down and felt for a pulse, for the faintest stir of breath.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah2 = \"I am so very happy. I am thrilled. But then I hate the cruelty of death, destruction, decomposition. And now I feel sad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_sentence_len2(text):\n",
    "    word_counts = []\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sent_detect = PunktSentenceTokenizer()\n",
    "    \n",
    "    sentences = sent_detect.sentences_from_text(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        word_counts.append(len(words))\n",
    "    avg_word_count = sum(word_counts)/len(word_counts)  \n",
    "    return avg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen, bruised, and bloody as she was, I knew her.\n",
      "Her name was Seivarden Vendaai!\n",
      "And a long time ago she had been one of my officers?\n",
      "a young lieutenant, eventually promoted to her own command, another ship.\n",
      "I had thought her a thousand years dead, but she was, undeniably, here.\n",
      "I crouched down and felt for a pulse, for the faintest stir of breath.\n"
     ]
    }
   ],
   "source": [
    "avg_sentence_len2(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_len2(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am so very happy. 1.0 1.0\n",
      "I am thrilled. 0.6 0.7\n",
      "But then I hate the cruelty of death, destruction, decomposition. -0.8 0.9\n",
      "And now I feel sad. -0.5 1.0\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "n_phrases = blob.noun_phrases   # WordList(['titular threat', 'blob',\n",
    "for sentence in blob.sentences:\n",
    "    print sentence, (sentence.sentiment.polarity),(sentence.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  Frozen, bruised, and bloody as she was, I knew...\n",
       "1  I am so very happy. I am thrilled. But then I ...\n",
       "2  Frozen, bruised, and bloody as she was, I knew...\n",
       "3  I am so very happy. I am thrilled. But then I ..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geez = {'body':[blah,blah2,blah,blah2]}\n",
    "test = pd.DataFrame(geez)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(test)):\n",
    "    blob = TextBlob(test.ix[i,0])\n",
    "    tags = blob.tags\n",
    "    df_tags = pd.DataFrame(tags)\n",
    "    df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "    for x in range(0,len(df_tags)):\n",
    "            test.ix[i, df_tags.ix[x,1] ] = df_tags.ix[x,0]\n",
    "            \n",
    "test.fillna(0,inplace=True)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>IN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJS</th>\n",
       "      <th>NN</th>\n",
       "      <th>NNP</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PRP</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>RB</th>\n",
       "      <th>TO</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frozen, bruised, and bloody as she was, I knew...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am so very happy. I am thrilled. But then I ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   CC   CD   DT   IN   JJ  \\\n",
       "0  Frozen, bruised, and bloody as she was, I knew...  4.0  2.0  6.0  6.0  4.0   \n",
       "1  I am so very happy. I am thrilled. But then I ...  2.0  0.0  1.0  1.0  2.0   \n",
       "2  Frozen, bruised, and bloody as she was, I knew...  4.0  2.0  6.0  6.0  4.0   \n",
       "3  I am so very happy. I am thrilled. But then I ...  2.0  0.0  1.0  1.0  2.0   \n",
       "\n",
       "   JJS   NN  NNP  NNS  PRP  PRP$   RB   TO  VBD  VBN  VBP  \n",
       "0  1.0  9.0  3.0  2.0  7.0   4.0  4.0  1.0  9.0  3.0  0.0  \n",
       "1  0.0  4.0  0.0  0.0  4.0   0.0  4.0  0.0  0.0  1.0  4.0  \n",
       "2  1.0  9.0  3.0  2.0  7.0   4.0  4.0  1.0  9.0  3.0  0.0  \n",
       "3  0.0  4.0  0.0  0.0  4.0   0.0  4.0  0.0  0.0  1.0  4.0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PRP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RB</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VBN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VBP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  0\n",
       "0   CC  2\n",
       "1   DT  1\n",
       "2   IN  1\n",
       "3   JJ  2\n",
       "4   NN  4\n",
       "5  PRP  4\n",
       "6   RB  4\n",
       "7  VBN  1\n",
       "8  VBP  4"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(blah2)\n",
    "tags = blob.tags\n",
    "df_tags = pd.DataFrame(tags)\n",
    "df_tags = df_tags.groupby([1]).count().reset_index()\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.9398972263774801, p_neg=0.060102773622521886)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so very happy.\n",
      "Sentiment(classification='pos', p_pos=0.6188007636620464, p_neg=0.38119923633795355)\n",
      "I am thrilled.\n",
      "Sentiment(classification='pos', p_pos=0.7187500000000001, p_neg=0.2812500000000001)\n",
      "But then I hate the cruelty of death, destruction, decomposition.\n",
      "Sentiment(classification='pos', p_pos=0.7782193562862965, p_neg=0.22178064371370343)\n",
      "And now I feel sad.\n",
      "Sentiment(classification='pos', p_pos=0.5571903253721332, p_neg=0.4428096746278663)\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(blah2, analyzer=NaiveBayesAnalyzer())\n",
    "for sentence in blob.sentences:\n",
    "    blob = TextBlob(str(sentence),analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "    print sentence \n",
    "    print blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epub_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_epub_working/\"\n",
    "txt_path = \"/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/_txt/\"\n",
    "path = '/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/'\n",
    "folders = ['sci-fi_top','sci-fi_flop','romance_top','romance_flop']\n",
    "folders2 = ['sci-fi_top','sci-fi_flop','romance_top','romance_flop','/test_hold_out/sci-fi_top','/test_hold_out/sci-fi_flop',\n",
    "           '/test_hold_out/romance_top','/test_hold_out/romance_flop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = pd.DataFrame()\n",
    "for folder in folders2:\n",
    "    for text_file in os.listdir(path+folder+'/'):\n",
    "        if text_file.endswith((\".txt\")):\n",
    "            \n",
    "            new_name = text_file[:text_file.find('-')]\n",
    "            author = text_file[text_file.find('-')+1:text_file.find('.')]\n",
    "\n",
    "            \n",
    "            temp = pd.DataFrame({\n",
    "                    'title': new_name.replace('_',' '),\n",
    "                    'author': author.replace('_',' ')\n",
    "                }, index=[0])\n",
    "            titles = pd.concat([titles, temp])\n",
    "            \n",
    "#titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles.to_csv(path + 'title_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIN\n",
      "WIN\n"
     ]
    }
   ],
   "source": [
    "from pattern.web    import Twitter\n",
    "from pattern.en     import tag\n",
    "from pattern.vector import KNN, count\n",
    "\n",
    "twitter, knn = Twitter(), KNN()\n",
    "\n",
    "for i in range(1, 3):\n",
    "    for tweet in twitter.search('#win OR #fail', start=i, count=100):\n",
    "        s = tweet.text.lower()\n",
    "        p = '#win' in s and 'WIN' or 'FAIL'\n",
    "        v = tag(s)\n",
    "        v = [word for word, pos in v if pos == 'JJ'] # JJ = adjective\n",
    "        v = count(v) # {'sweet': 1}\n",
    "        if v:\n",
    "            knn.train(v, type=p)\n",
    "\n",
    "print knn.classify('sweet potato burger')\n",
    "print knn.classify('stupid autocorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so very happy. I am thrilled. But then I hate the cruelty of death, destruction, decomposition. And now I feel sad.\n",
      "\n",
      "indicative\n",
      "0.25\n",
      "\n",
      "Sentence('I/PRP/B-NP/O/i am/VBP/B-VP/O/be so/RB/B-ADJP/O/so very/RB/I-ADJP/O/very happy/JJ/I-ADJP/O/happy ././O/O/.\\nI am/VBP/B-VP/O/be thrilled/VBN/I-VP/O/thrill ././O/O/.\\nBut then/RB/B-ADVP/O/then I/PRP/B-NP/O/i hate/VBP/B-VP/O/hate the/DT/B-NP/O/the cruelty/NN/I-NP/O/cruelty of/IN/B-PP/B-PNP/of death/NN/B-NP/I-PNP/death ,/,/O/O/, destruction/NN/B-NP/O/destruction ,/,/O/O/, decomposition/NN/B-NP/O/decomposition ././O/O/.\\nAnd now/RB/B-ADVP/O/now I/PRP/B-NP/O/i feel/VBP/B-VP/O/feel sad/JJ/B-ADJP/O/sad ././O/O/.')\n",
      "indicative\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse, Sentence, parse, modality, mood, wordnet \n",
    "# mood() function returns either INDICATIVE, IMPERATIVE, CONDITIONAL or SUBJUNCTIVE\n",
    "# modality() function returns the degree of certainty as a value between -1.0 and +1.0, \n",
    "# where values > +0.5 represent facts. For example, \"I wish it would stop raining\" scores -0.35, \n",
    "# whereas \"It will stop raining\" scores +0.75. \n",
    "# Accuracy is about 68% for Wikipedia texts.\n",
    "\n",
    "print blah2\n",
    "print\n",
    "print mood(blah2)\n",
    "print modality(blah2)\n",
    "print \n",
    "s = parse(blah2, lemmata=True)\n",
    "s = Sentence(s)\n",
    "print s\n",
    "print mood(s)\n",
    "print modality(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Synset.lexname of Synset('drive.v.01')>\n",
      "<bound method Synset.lexname of Synset('drive.v.02')>\n",
      "<bound method Synset.lexname of Synset('drive.v.03')>\n",
      "<bound method Synset.lexname of Synset('force.v.06')>\n",
      "<bound method Synset.lexname of Synset('drive.v.05')>\n",
      "<bound method Synset.lexname of Synset('repel.v.01')>\n",
      "<bound method Synset.lexname of Synset('drive.v.07')>\n",
      "<bound method Synset.lexname of Synset('drive.v.08')>\n",
      "<bound method Synset.lexname of Synset('drive.v.09')>\n",
      "<bound method Synset.lexname of Synset('tug.v.02')>\n",
      "<bound method Synset.lexname of Synset('drive.v.11')>\n",
      "<bound method Synset.lexname of Synset('drive.v.12')>\n",
      "<bound method Synset.lexname of Synset('drive.v.13')>\n",
      "<bound method Synset.lexname of Synset('drive.v.14')>\n",
      "<bound method Synset.lexname of Synset('drive.v.15')>\n",
      "<bound method Synset.lexname of Synset('drive.v.16')>\n",
      "<bound method Synset.lexname of Synset('drive.v.17')>\n",
      "<bound method Synset.lexname of Synset('drive.v.18')>\n",
      "<bound method Synset.lexname of Synset('drive.v.19')>\n",
      "<bound method Synset.lexname of Synset('drive.v.20')>\n",
      "<bound method Synset.lexname of Synset('drive.v.21')>\n",
      "<bound method Synset.lexname of Synset('drive.v.22')>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "for synset in wn.synsets('drive','v'):\n",
    "    print  synset.lexname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "29\tverb.body\tverbs of grooming, dressing and bodily care\n",
    "30\tverb.change\tverbs of size, temperature change, intensifying, etc.\n",
    "31\tverb.cognition\tverbs of thinking, judging, analyzing, doubting\n",
    "32\tverb.communication\tverbs of telling, asking, ordering, singing\n",
    "33\tverb.competition\tverbs of fighting, athletic activities\n",
    "34\tverb.consumption\tverbs of eating and drinking\n",
    "35\tverb.contact\tverbs of touching, hitting, tying, digging\n",
    "36\tverb.creation\tverbs of sewing, baking, painting, performing\n",
    "37\tverb.emotion\tverbs of feeling\n",
    "38\tverb.motion\tverbs of walking, flying, swimming\n",
    "39\tverb.perception\tverbs of seeing, hearing, feeling\n",
    "40\tverb.possession\tverbs of buying, selling, owning\n",
    "41\tverb.social\tverbs of political and social activities and events\n",
    "42\tverb.stative\tverbs of being, having, spatial relations\n",
    "43\tverb.weather\tverbs of raining, snowing, thawing, thundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#words = tokenizer.tokenize(text)\n",
    "# for i in range(1, 3):\n",
    "#     for tweet in twitter.search('#win OR #fail', start=i, count=100):\n",
    "lower_blah = blah.lower()\n",
    "        #p = '#win' in s and 'WIN' or 'FAIL'\n",
    "v = tag(lower_blah)\n",
    "v = [word for word, pos in v if pos[:2] == 'VB'] # VB* = verb\n",
    "#v = count(v) \n",
    "    #if v:\n",
    "            #knn.train(v, type=p)\n",
    "\n",
    "#print knn.classify('sweet potato burger')\n",
    "#print knn.classify('stupid autocorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruised <bound method Synset.lexname of Synset('bruise.v.01')>\n",
      "bruised <bound method Synset.lexname of Synset('hurt.v.05')>\n",
      "bruised <bound method Synset.lexname of Synset('bruise.v.03')>\n",
      "bruised <bound method Synset.lexname of Synset('bruise.v.04')>\n",
      "was <bound method Synset.lexname of Synset('be.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.03')>\n",
      "was <bound method Synset.lexname of Synset('exist.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.05')>\n",
      "was <bound method Synset.lexname of Synset('equal.v.01')>\n",
      "was <bound method Synset.lexname of Synset('constitute.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.08')>\n",
      "was <bound method Synset.lexname of Synset('embody.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.10')>\n",
      "was <bound method Synset.lexname of Synset('be.v.11')>\n",
      "was <bound method Synset.lexname of Synset('be.v.12')>\n",
      "was <bound method Synset.lexname of Synset('cost.v.01')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.01')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.02')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.03')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.04')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.05')>\n",
      "knew <bound method Synset.lexname of Synset('acknowledge.v.06')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.07')>\n",
      "knew <bound method Synset.lexname of Synset('sleep_together.v.01')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.09')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.10')>\n",
      "knew <bound method Synset.lexname of Synset('know.v.11')>\n",
      "was <bound method Synset.lexname of Synset('be.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.03')>\n",
      "was <bound method Synset.lexname of Synset('exist.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.05')>\n",
      "was <bound method Synset.lexname of Synset('equal.v.01')>\n",
      "was <bound method Synset.lexname of Synset('constitute.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.08')>\n",
      "was <bound method Synset.lexname of Synset('embody.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.10')>\n",
      "was <bound method Synset.lexname of Synset('be.v.11')>\n",
      "was <bound method Synset.lexname of Synset('be.v.12')>\n",
      "was <bound method Synset.lexname of Synset('cost.v.01')>\n",
      "had <bound method Synset.lexname of Synset('have.v.01')>\n",
      "had <bound method Synset.lexname of Synset('have.v.02')>\n",
      "had <bound method Synset.lexname of Synset('experience.v.03')>\n",
      "had <bound method Synset.lexname of Synset('own.v.01')>\n",
      "had <bound method Synset.lexname of Synset('get.v.03')>\n",
      "had <bound method Synset.lexname of Synset('consume.v.02')>\n",
      "had <bound method Synset.lexname of Synset('have.v.07')>\n",
      "had <bound method Synset.lexname of Synset('hold.v.03')>\n",
      "had <bound method Synset.lexname of Synset('have.v.09')>\n",
      "had <bound method Synset.lexname of Synset('have.v.10')>\n",
      "had <bound method Synset.lexname of Synset('have.v.11')>\n",
      "had <bound method Synset.lexname of Synset('have.v.12')>\n",
      "had <bound method Synset.lexname of Synset('induce.v.02')>\n",
      "had <bound method Synset.lexname of Synset('accept.v.02')>\n",
      "had <bound method Synset.lexname of Synset('receive.v.01')>\n",
      "had <bound method Synset.lexname of Synset('suffer.v.02')>\n",
      "had <bound method Synset.lexname of Synset('have.v.17')>\n",
      "had <bound method Synset.lexname of Synset('give_birth.v.01')>\n",
      "had <bound method Synset.lexname of Synset('take.v.35')>\n",
      "been <bound method Synset.lexname of Synset('be.v.01')>\n",
      "been <bound method Synset.lexname of Synset('be.v.02')>\n",
      "been <bound method Synset.lexname of Synset('be.v.03')>\n",
      "been <bound method Synset.lexname of Synset('exist.v.01')>\n",
      "been <bound method Synset.lexname of Synset('be.v.05')>\n",
      "been <bound method Synset.lexname of Synset('equal.v.01')>\n",
      "been <bound method Synset.lexname of Synset('constitute.v.01')>\n",
      "been <bound method Synset.lexname of Synset('be.v.08')>\n",
      "been <bound method Synset.lexname of Synset('embody.v.02')>\n",
      "been <bound method Synset.lexname of Synset('be.v.10')>\n",
      "been <bound method Synset.lexname of Synset('be.v.11')>\n",
      "been <bound method Synset.lexname of Synset('be.v.12')>\n",
      "been <bound method Synset.lexname of Synset('cost.v.01')>\n",
      "promoted <bound method Synset.lexname of Synset('promote.v.01')>\n",
      "promoted <bound method Synset.lexname of Synset('promote.v.02')>\n",
      "promoted <bound method Synset.lexname of Synset('advertise.v.02')>\n",
      "promoted <bound method Synset.lexname of Synset('promote.v.04')>\n",
      "promoted <bound method Synset.lexname of Synset('promote.v.05')>\n",
      "had <bound method Synset.lexname of Synset('have.v.01')>\n",
      "had <bound method Synset.lexname of Synset('have.v.02')>\n",
      "had <bound method Synset.lexname of Synset('experience.v.03')>\n",
      "had <bound method Synset.lexname of Synset('own.v.01')>\n",
      "had <bound method Synset.lexname of Synset('get.v.03')>\n",
      "had <bound method Synset.lexname of Synset('consume.v.02')>\n",
      "had <bound method Synset.lexname of Synset('have.v.07')>\n",
      "had <bound method Synset.lexname of Synset('hold.v.03')>\n",
      "had <bound method Synset.lexname of Synset('have.v.09')>\n",
      "had <bound method Synset.lexname of Synset('have.v.10')>\n",
      "had <bound method Synset.lexname of Synset('have.v.11')>\n",
      "had <bound method Synset.lexname of Synset('have.v.12')>\n",
      "had <bound method Synset.lexname of Synset('induce.v.02')>\n",
      "had <bound method Synset.lexname of Synset('accept.v.02')>\n",
      "had <bound method Synset.lexname of Synset('receive.v.01')>\n",
      "had <bound method Synset.lexname of Synset('suffer.v.02')>\n",
      "had <bound method Synset.lexname of Synset('have.v.17')>\n",
      "had <bound method Synset.lexname of Synset('give_birth.v.01')>\n",
      "had <bound method Synset.lexname of Synset('take.v.35')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.01')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.02')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.03')>\n",
      "thought <bound method Synset.lexname of Synset('remember.v.01')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.05')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.06')>\n",
      "thought <bound method Synset.lexname of Synset('intend.v.01')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.08')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.09')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.10')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.11')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.12')>\n",
      "thought <bound method Synset.lexname of Synset('think.v.13')>\n",
      "was <bound method Synset.lexname of Synset('be.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.03')>\n",
      "was <bound method Synset.lexname of Synset('exist.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.05')>\n",
      "was <bound method Synset.lexname of Synset('equal.v.01')>\n",
      "was <bound method Synset.lexname of Synset('constitute.v.01')>\n",
      "was <bound method Synset.lexname of Synset('be.v.08')>\n",
      "was <bound method Synset.lexname of Synset('embody.v.02')>\n",
      "was <bound method Synset.lexname of Synset('be.v.10')>\n",
      "was <bound method Synset.lexname of Synset('be.v.11')>\n",
      "was <bound method Synset.lexname of Synset('be.v.12')>\n",
      "was <bound method Synset.lexname of Synset('cost.v.01')>\n",
      "crouched <bound method Synset.lexname of Synset('crouch.v.01')>\n",
      "crouched <bound method Synset.lexname of Synset('squat.v.01')>\n",
      "felt <bound method Synset.lexname of Synset('felt.v.01')>\n",
      "felt <bound method Synset.lexname of Synset('felt.v.02')>\n",
      "felt <bound method Synset.lexname of Synset('felt.v.03')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.01')>\n",
      "felt <bound method Synset.lexname of Synset('find.v.05')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.03')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.04')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.05')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.06')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.07')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.08')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.09')>\n",
      "felt <bound method Synset.lexname of Synset('palpate.v.01')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.11')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.12')>\n",
      "felt <bound method Synset.lexname of Synset('feel.v.13')>\n"
     ]
    }
   ],
   "source": [
    "for i in v:\n",
    "    for synset in wn.synsets(i,'v'):\n",
    "        print  i, synset.lexname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/katbishop/Desktop/DSI-SF2-bishopkd/projects/capstone/data/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import verbnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs2=[]\n",
    "for i in range(0,len(df)/2):\n",
    "    words = nltk.word_tokenize(df.ix[i,'body'])\n",
    "    for word,pos in nltk.pos_tag(words):\n",
    "        if pos[:2] == 'VB': #Another way to focus on only verbs\n",
    "            verbs2.append(word+'-'+pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31690"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_set = list(set(verbs))\n",
    "len(verb_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1574348"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object <genexpr> at 0x123006320>]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'fawn',\n",
       " u'remorse.so',\n",
       " u'half-wrestl',\n",
       " u'circuitri',\n",
       " u'clot',\n",
       " u'hang',\n",
       " u'trawl',\n",
       " u'un-tuck',\n",
       " u'local',\n",
       " u'disobey',\n",
       " u'mutini',\n",
       " u'last',\n",
       " u'adjust',\n",
       " u'scold',\n",
       " u'wrong-foot',\n",
       " u'cane',\n",
       " u'lead',\n",
       " u'wrack',\n",
       " u'therewasnt',\n",
       " u'stipul',\n",
       " u'scream',\n",
       " u'wood',\n",
       " u'gruel',\n",
       " u'wooden',\n",
       " u'iwei',\n",
       " u'broil',\n",
       " u'crotch',\n",
       " u'assumethat',\n",
       " u'fester',\n",
       " u'carolineanyth',\n",
       " u'snuggl',\n",
       " u'waistwa',\n",
       " u'scrape',\n",
       " u'shock',\n",
       " u'three',\n",
       " u'hador',\n",
       " u'implod',\n",
       " u'sustain',\n",
       " u'consent',\n",
       " u'reengin']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vstems = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "for word in verb_set:\n",
    "    stem = stemmer.stem(word)\n",
    "    vstems.append(stem.lower())\n",
    "\n",
    "vstems[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vstems = list(set(vstems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16536"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vstems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (path+\"stem.csv\",\"w\") as fp:\n",
    "    for line in vstems:\n",
    "        fp.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolv\n",
      "absorb\n",
      "absorb\n",
      "absorb\n",
      "absorb\n",
      "abstain\n",
      "abstain\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "test = ['absolved','absorb','absorbed','absorbing','absorbs','abstain','abstained']\n",
    "for word in test:\n",
    "    print stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
